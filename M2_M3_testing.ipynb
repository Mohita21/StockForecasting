{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook two models are tested:\n",
    "### 1. Statistical model: This model considers all the price features and forecasts for 5 days ahead. Vector Autoregression Model is used for multivariate forecasting.\n",
    "### 2. DL Model: This model has GRU layers and is used for univariate forecasting. The closing price is converted into a sliding window of past 20 days and mapped to 5 days ahead closing value for forecast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# For reading stock data from yahoo\n",
    "from pandas_datareader.data import DataReader\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU\n",
    "import keras\n",
    "from tensorflow.keras import initializers\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "import os\n",
    "from statsmodels.tsa.api import VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"companies\": {\n",
    "        \"tickers\": ['BNS', 'RY', 'TD', 'BMO', 'CM', 'C', 'JPM', 'IBN', 'WTBA', 'BAC', 'AXP',\n",
    "       'PNC'],\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"test_len\": 5,\n",
    "        \"window\": 25,\n",
    "        \"features\": 1\n",
    "    }, \n",
    "    \"model\": {\n",
    "        \"num_lstm_layers\": 2,\n",
    "        \"lstm_size\": 32,\n",
    "        \"dropout\": 0.2,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 50\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_transformation(X_train, pred):\n",
    "    forecast = pred.copy()\n",
    "    columns = X_train.columns\n",
    "    for col in columns:\n",
    "        forecast[str(col)+'_pred'] = X_train[col].iloc[-1] + forecast[str(col)+'_pred'].cumsum()\n",
    "    return forecast\n",
    "\n",
    "def statistical_model(df):\n",
    "    X_train=df[:-5]\n",
    "    X_test=df[-5:]\n",
    "    X_diff=X_train.diff().dropna()\n",
    "    mod = VAR(X_diff)\n",
    "    res = mod.fit(maxlags=30, ic='aic')\n",
    "    #print(res.summary())\n",
    "    y_fitted = res.fittedvalues\n",
    "    lag_order = res.k_ar\n",
    "    input_data = X_diff.values[-lag_order:]\n",
    "    pred = res.forecast(y=input_data, steps=5)\n",
    "    pred = pd.DataFrame(pred, index=X_test.index, columns=X_test.columns + '_pred')\n",
    "    output = invert_transformation(X_train, pred)\n",
    "    #print(output)\n",
    "    out=output[\"Close_pred\"]\n",
    "    #print(out)\n",
    "    o1=out[-1:]\n",
    "    o2=df[\"Close\"][-1:]\n",
    "    prediction=output[\"Close_pred\"].values\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting is stopped using callbacks like early stopping and a validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(ticker):\n",
    "    df = DataReader(ticker, data_source='yahoo', start='2015-01-01', end=datetime.now())\n",
    "    return df\n",
    "\n",
    "def data_processing(config,df):\n",
    "    d_c = df.filter(['Close'])\n",
    "    data = d_c.values\n",
    "    train_len = len(data)-config[\"data\"][\"test_len\"]\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    train_data = scaled_data[0:int(train_len), :]\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for i in range(config[\"data\"][\"window\"], len(train_data)):\n",
    "        x_train.append(train_data[i-config[\"data\"][\"window\"]:i, 0])\n",
    "        y_train.append(train_data[i, 0])\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    #print(x_train.shape)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    test_data = scaled_data[train_len - config[\"data\"][\"window\"]: , :]\n",
    "    x_test = []\n",
    "    y_test = data[train_len:, :]\n",
    "    for i in range(config[\"data\"][\"window\"], len(test_data)):\n",
    "        x_test.append(test_data[i-config[\"data\"][\"window\"]:i, 0])\n",
    "    x_test = np.array(x_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n",
    "    #print(x_train.shape)\n",
    "    #print(x_test.shape)\n",
    "    return x_train,y_train,x_test,y_test, scaler\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_dl(lag,steps_ahead,df):\n",
    "    df = df.filter(['Close']).values\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    df = scaler.fit_transform(df)\n",
    "    y=df[lag+steps_ahead-1:]\n",
    "    y_train=y[:-test_len]\n",
    "    y_test=y[-test_len:]\n",
    "    x=series_to_supervised_dl(df,lag-1)\n",
    "    x_train=x[:len(y_train)].values\n",
    "    x_test=x[-test_len-steps_ahead:-steps_ahead].values\n",
    "    x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "    x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "    return x_train,x_test,y_train,y_test,scaler\n",
    "\n",
    "\n",
    "\n",
    "def model(config):\n",
    "    os.environ['PYTHONHASHSEED'] = '42'\n",
    "    np.random.seed(42)\n",
    "    np.random.RandomState(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(GRU(128, return_sequences=True, input_shape= (config[\"data\"][\"window\"], config[\"data\"][\"features\"])))\n",
    "    #model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "    model.add(GRU(64, return_sequences=False))\n",
    "    model.add(Dense(25))\n",
    "    model.add(Dense(1))\n",
    "    return model\n",
    "\n",
    "def train_univariate(config,model,x_train,y_train,path):\n",
    "    reduce_lr = keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
    "    checkpoint1 = keras.callbacks.ModelCheckpoint(path,\n",
    "                                                     monitor='val_loss',\n",
    "                                                     save_best_only=True,\n",
    "                                                     mode='min')\n",
    "    es=keras.callbacks.EarlyStopping(patience=5)\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    history=model.fit(x_train,y_train,epochs=config[\"training\"][\"epochs\"],validation_split=0.2,verbose=0,batch_size=config[\"training\"][\"batch_size\"],callbacks=[reduce_lr,checkpoint1,es])\n",
    "\n",
    "\n",
    "    \n",
    "def series_to_supervised_dl(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "    \n",
    "    \n",
    "def predict_dl(model,path,x_test,y_test,scaler):\n",
    "    model.load_weights(path)\n",
    "    predictions = model.predict(x_test)\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    #print(valid)\n",
    "    return predictions\n",
    "\n",
    "def check_worth_increase(config,predictions,df):\n",
    "    past_close=df.filter([\"Close\"])[-config[\"data\"][\"test_len\"]-1:-config[\"data\"][\"test_len\"]]\n",
    "    #print(df.filter([\"Close\"]))\n",
    "    #print(\"The predicted value for 5 days ahead is \",predictions[-1:], \"the actual value is \",df.filter([\"Close\"])[-1:])\n",
    "    if predictions[-1:] > past_close.values[0]:\n",
    "        print(\"future value {} greater than past value {}\".format(predictions[-1:],past_close.values))\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"future value {} lesser than past value {}\".format(predictions[-1:],past_close.values))\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def true_worth_increase(config,df):\n",
    "    past_close=df.filter([\"Close\"])[-config[\"data\"][\"test_len\"]-1:-config[\"data\"][\"test_len\"]]\n",
    "    #print(df.filter([\"Close\"]))\n",
    "    #print(\"The predicted value for 5 days ahead is \",predictions[-1:], \"the actual value is \",df.filter([\"Close\"])[-1:])\n",
    "    if df.filter([\"Close\"])[-1:].values > past_close.values:\n",
    "        print(\"future actual value {} greater than past value {}\".format(df.filter([\"Close\"])[-1:],past_close.values))\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"future actual value {} lesser than past value {}\".format(df.filter([\"Close\"])[-1:],past_close.values))\n",
    "        return 0   \n",
    "    \n",
    "def test(pred,true):\n",
    "    rmse = np.sqrt(np.mean(((pred - true) ** 2)))\n",
    "    mae = mean_absolute_error(pred, true)\n",
    "    mape= np.mean(mae/true) *100\n",
    "    return rmse, mae, mape\n",
    "  \n",
    "def sliding_data(df,index):\n",
    "    df=df[:-index]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing the models\n",
    "## A sliding window testing technique is used in which the models are trained and tested 10 times and data is sliced in a way to test for 5 days ahead forecast.\n",
    "\n",
    "## Thus the model is tested on 50 data points which is the data for 10 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " For: BNS\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "2.1967088384472273\n",
      "1.4477498917343106\n",
      " \n",
      " \n",
      " For: RY\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "2.29872083478817\n",
      "1.1580047450664446\n",
      " \n",
      " \n",
      " For: TD\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "2.406784667147635\n",
      "1.0908419954294595\n",
      " \n",
      " \n",
      " For: BMO\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "2.8068312532909467\n",
      "1.350441816536779\n",
      " \n",
      " \n",
      " For: CM\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "2.844227486774407\n",
      "1.2829042912326665\n",
      " \n",
      " \n",
      " For: C\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "4.059731016825612\n",
      "2.83471534073952\n",
      " \n",
      " \n",
      " For: JPM\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "2.787255421690293\n",
      "2.22797616570145\n",
      " \n",
      " \n",
      " For: IBN\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "3.103551246184454\n",
      "2.2445188226719353\n",
      " \n",
      " \n",
      " For: WTBA\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "3.620857605903092\n",
      "2.8619009036798686\n",
      " \n",
      " \n",
      " For: BAC\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "4.004264276108156\n",
      "2.821382068602397\n",
      " \n",
      " \n",
      " For: AXP\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "3.2028985737675786\n",
      "2.9454327680147605\n",
      " \n",
      " \n",
      " For: PNC\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "3.6467104367381022\n",
      "2.856768991732748\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DL,STAT=[],[]\n",
    "lag = 25\n",
    "steps_ahead = 5\n",
    "n_features = 6\n",
    "test_len=5\n",
    "\n",
    "for tick in config[\"companies\"][\"tickers\"]:\n",
    "    print(\" \\n \\n For:\",tick)\n",
    "    df=get_data(tick)\n",
    "    metric_dl, metric_stat=[],[]\n",
    "    for idx in range(5,50,5):  \n",
    "        true=df[\"Close\"][-test_len:].values\n",
    "        path=\"bestweight\"+tick+\".hdf5\" \n",
    "        x_train,x_test,y_train,y_test,scaler=preprocess_dl(lag,steps_ahead,df)\n",
    "        #print(x_train.shape)\n",
    "        #print(y_train.shape)\n",
    "        model1=model(config)\n",
    "        print(\"Training...\")\n",
    "        train_univariate(config,model1,x_train,y_train,path)\n",
    "        predictions=predict_dl(model1,path,x_test,y_test,scaler)\n",
    "        rmse, mae, mape=test(predictions,true)\n",
    "        metric_dl.append([rmse, mae, mape])\n",
    "        pred=statistical_model(df)\n",
    "        rmse, mae, mape=test(pred,true)\n",
    "        metric_stat.append([rmse, mae, mape])\n",
    "        df=sliding_data(df,idx)\n",
    "    df_DL = DataFrame(metric_dl,columns=['RMSE','MAE','MAPE'])\n",
    "    print(np.mean(df_DL[\"MAPE\"]))\n",
    "    df_STAT = DataFrame(metric_stat,columns=['RMSE','MAE','MAPE'])\n",
    "    print(np.mean(df_STAT[\"MAPE\"]))\n",
    "    STAT.append(np.mean(df_STAT[\"MAPE\"]))\n",
    "    DL.append(np.mean(df_DL[\"MAPE\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The values below are the Mean Absolute Percentage Error values obtained for all the banks when tested using the sliding window testing 10 times for 5 values for the Statistical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BNS</th>\n",
       "      <th>RY</th>\n",
       "      <th>TD</th>\n",
       "      <th>BMO</th>\n",
       "      <th>CM</th>\n",
       "      <th>C</th>\n",
       "      <th>JPM</th>\n",
       "      <th>IBN</th>\n",
       "      <th>WTBA</th>\n",
       "      <th>BAC</th>\n",
       "      <th>AXP</th>\n",
       "      <th>PNC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.44775</td>\n",
       "      <td>1.158005</td>\n",
       "      <td>1.090842</td>\n",
       "      <td>1.350442</td>\n",
       "      <td>1.282904</td>\n",
       "      <td>2.834715</td>\n",
       "      <td>2.227976</td>\n",
       "      <td>2.244519</td>\n",
       "      <td>2.861901</td>\n",
       "      <td>2.821382</td>\n",
       "      <td>2.945433</td>\n",
       "      <td>2.856769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       BNS        RY        TD       BMO        CM         C       JPM  \\\n",
       "0  1.44775  1.158005  1.090842  1.350442  1.282904  2.834715  2.227976   \n",
       "\n",
       "        IBN      WTBA       BAC       AXP       PNC  \n",
       "0  2.244519  2.861901  2.821382  2.945433  2.856769  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STAT_mape=pd.DataFrame(STAT).T\n",
    "STAT_mape.columns=config[\"companies\"][\"tickers\"]\n",
    "STAT_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The average MAPE of the this statistical model is around 2 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.093553150095195"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(STAT).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The values below are the Mean Absolute Percentage Error values obtained for all the banks when tested using the sliding window testing 10 times for 5 values for the DL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BNS</th>\n",
       "      <th>RY</th>\n",
       "      <th>TD</th>\n",
       "      <th>BMO</th>\n",
       "      <th>CM</th>\n",
       "      <th>C</th>\n",
       "      <th>JPM</th>\n",
       "      <th>IBN</th>\n",
       "      <th>WTBA</th>\n",
       "      <th>BAC</th>\n",
       "      <th>AXP</th>\n",
       "      <th>PNC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.196709</td>\n",
       "      <td>2.298721</td>\n",
       "      <td>2.406785</td>\n",
       "      <td>2.806831</td>\n",
       "      <td>2.844227</td>\n",
       "      <td>4.059731</td>\n",
       "      <td>2.787255</td>\n",
       "      <td>3.103551</td>\n",
       "      <td>3.620858</td>\n",
       "      <td>4.004264</td>\n",
       "      <td>3.202899</td>\n",
       "      <td>3.64671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        BNS        RY        TD       BMO        CM         C       JPM  \\\n",
       "0  2.196709  2.298721  2.406785  2.806831  2.844227  4.059731  2.787255   \n",
       "\n",
       "        IBN      WTBA       BAC       AXP      PNC  \n",
       "0  3.103551  3.620858  4.004264  3.202899  3.64671  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DL_mape=pd.DataFrame(DL).T\n",
    "DL_mape.columns=config[\"companies\"][\"tickers\"]\n",
    "DL_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The average MAPE of the this DL model is around 3 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.081545138138807"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(DL).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
